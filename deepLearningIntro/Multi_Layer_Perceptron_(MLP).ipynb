{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_Layer_Perceptron_(MLP).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKzPGNNIbnzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " %tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24EckoXRb1aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP:\n",
        "  def __init__(self, seq_max_len, state_size, vocab_size, num_classes):\n",
        "    self.seq_max_len = seq_max_len\n",
        "    self.state_size = state_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_classes = num_classes\n",
        "    \n",
        "  def build_model(self):\n",
        "    self.x = tf.placeholder(shape=[None, self.seq_max_len], dtype=tf.int32)\n",
        "    x_one_hot = tf.one_hot(self.x, self.vocab_size)\n",
        "    x_one_hot = tf.cast(x_one_hot, tf.float32)\n",
        "\n",
        "    self.y = tf.placeholder(shape=[None], dtype=tf.int32)\n",
        "    self.y_onehot = tf.one_hot(self.y, self.num_classes, dtype=tf.float32)\n",
        "\n",
        "    self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "\n",
        "    weights = {\n",
        "        'layer_0': tf.Variable(tf.random_normal([self.seq_max_len*self.vocab_size, self.state_size])),\n",
        "        'layer_1': tf.Variable(tf.random_normal([self.state_size, self.state_size])),\n",
        "        'layer_2': tf.Variable(tf.random_normal([self.state_size, self.num_classes]))\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'layer_0': tf.Variable(tf.random_normal([self.state_size])),\n",
        "        'layer_1': tf.Variable(tf.random_normal([self.state_size])),\n",
        "        'layer_2': tf.Variable(tf.random_normal([self.num_classes]))\n",
        "    }\n",
        "\n",
        "    x_input = tf.reshape(x_one_hot, [-1, self.seq_max_len*self.vocab_size])\n",
        "\n",
        "    hidden = tf.matmul(x_input, weights['layer_0']) + biases['layer_0']\n",
        "    hidden = tf.nn.sigmoid(hidden)\n",
        "\n",
        "    hidden = tf.matmul(hidden, weights['layer_1']) + biases['layer_1']\n",
        "    hidden = tf.nn.sigmoid(hidden)\n",
        "\n",
        "    self.logits = tf.matmul(hidden, weights['layer_2']) + biases['layer_2']\n",
        "    self.probs = tf.nn.softmax(self.logits, axis=1)\n",
        "\n",
        "    self.correct_preds = tf.equal(tf.argmax(self.probs, axis=1), tf.argmax(self.y_onehot, axis=1))\n",
        "    self.precision = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
        "  \n",
        "  def step_training(self, learning_rate=0.01):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(label=self.y_onehot, logits=self.logits))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "    return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlRhnGqEmOdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}